---
layout: page
title: Pre-Processing
description: >
  Step 3 of the pipeline is pre-processing. How will you store this data?
hide_description: true
sitemap: false
---

Now that you have your data, you need to decide how to store it. Think about what attributes of the
data you will be analyzing, and how they can be stored and manipulated efficiently. However, you
don't need to figure out every little detail right now. If you get to a later step and realize that
you need easier access to certain attributes of the raw data, you can always come back here and
re-organize. The data science pipeline doesn't need to be linear!

## Tidy Data

One concept commonly associated with this step is that of *tidy data*. Data that is *tidy* is
stored in a way that most accurately follows the format of the observations it describes. It is
important to note that this may not be the most visually appealing or memory-efficient method of
storing the data, but it will make it easier to analyze with tools like `pandas`, as all of the
information you will need during your analysis will be readily available. It may also make handling
*missing data* much easier, which we will get to soon. If you would like to learn more about tidy
data, statistician Hadley Wickham wrote [an in-depth paper on the subject][00]. In this paper he
outlines the basic requirements of tidy data:
> 1. Each variable forms a column.
> 2. Each observation forms a row.
> 3. Each type of observational unit forms a table

For this project, the *variables* are the replay statistics generated by ballchasing.com, so they
will be the columns of my data table. The *observations* will be one player's side of each game. I
will be looking at 1v1 matches, so each game will have 2 observations. The only data I will need
is the match statistics, and due to their nature it makes sense for them to all be in the same
table. Thus I will have 1 *observational unit*, and it will be my 1 data table.

In [`pre_processing.py`][01] I defined some methods to aggregate and tidy my data. Let's take a
look at that file now.

### Imports

```python
# file: "pre_processing.py"
import os
import pandas as pd
import json as json_lib

from display_lib import ranks
```

`display_lib` is a file which I wrote for this project, and it contains common variables and helper
methods. It is available along with everything else in the [main project directory][01], and links
to explanations of its contents will be given as it is referenced. Here, `ranks` is just a list of
the colloquial names of the 8 ranks, in order from lowest to highest.

### Extracting the Data from the JSON Summaries

```python
# file: "pre_processing.py"
def iter_stats():
  '''A generator function which yields a `pd.Series` for each player in each summary in the
  subfolders of `summaries`.
  
  ## Yields:
  ((rank, guid, color), series)
    where rank is the rank of the match,
    guid is the unique id of the match,
    color is the team color of the yielded `Series`, and
    series is a `Series` containing all of the ballchasing statistics.
  '''
  for folder, _, files in [f for f in os.walk('summaries')][1:]:
    rank = folder.split('\\')[-1]

    for file in files:
      try:
        # read the match summary and find the guid
        with open(f'{folder}\\{file}') as f:
          json = json_lib.load(f)
        guid = json['match_guid']
        metadata = (json['title'], json['duration'])
  
        # for each team, yield a series containing all the stats
        for color in ['blue', 'orange']:
          series = pd.concat((
            pd.Series(metadata, index=('title', 'duration')), *[
              pd.Series(v, dtype=float)
            for v in json[color]['players'][0]['stats'].values()]
          ))
  
          # yield with tuple as key (for pd.MultiIndex)
          yield (rank, guid, color), series
      
      # skip the file if there is an error
      except Exception as e:
        print(f'Error with {folder}\\{file}: {e}')
```

This is a generator function designed to be used in the creation of a dictionary. The key of the
dictionary will be a 3-tuple containing the rank, unique id, and team color of a specific
observation. The value will be a `pandas.Series` containing all of the player statistics in the
JSON summary. It is useful to use a `Series` here since the names of the player metrics will be
retained in its indices. The processing of each file is enclosed in a `try ... except` in order to
catch `KeyError`s from indexing the JSON dictionary. This will be caused if any of the files are
not in the format we are expecting.

### Selecting the Desired Metrics

```python
# file: "pre_processing.py"
def prepare_metrics(data: pd.DataFrame):
  # calculate new metrics to replace some of the existing ones
  data.loc['amount_collected'] = data.loc['count_collected_big'] * 100 + data.loc['count_collected_small'] * 12
  data.loc['amount_stolen'] = data.loc['count_stolen_big'] * 100 + data.loc['count_stolen_small'] * 12

  # drop irrelevant or redundant metrics
  data.drop(labels=[
    'shots_against',                      # reciprocal of shots
    'goals_against',                      # reciprocal of goals
    'assists',                            # no assists in 1v1
    'mvp',                                # no mvp in 1v1
    'shooting_percentage',                # -> shots and goals
    'bcpm',                               # boost metrics will be normalized by duration
    'amount_collected_big',               # -> amount_collected
    'amount_collected_small',             # -> amount_collected
    'amount_stolen_big',                  # -> amount_stolen
    'amount_stolen_small',                # -> amount_stolen
    'count_collected_big',                # -> amount_collected
    'count_stolen_big',                   # -> amount_stolen
    'count_collected_small',              # -> amount_collected
    'count_stolen_small',                 # -> amount_stolen
    'amount_overfill',                    # represented in wasted metrics
    'amount_overfill_stolen',             # represented in wasted_stolen metrics
    'time_zero_boost',                    # percentage equivalent exists
    'time_full_boost',                    # percentage equivalent exists
    'time_boost_0_25',                    # not useful
    'time_boost_25_50',                   # not useful
    'time_boost_50_75',                   # not useful
    'time_boost_75_100',                  # not useful
    'percent_boost_0_25',                 # not useful
    'percent_boost_25_50',                # not useful
    'percent_boost_50_75',                # not useful
    'percent_boost_75_100',               # not useful
    'avg_speed',                          # percentage equivalent exists
    'total_distance',                     # -> avg_speed_percentage and duration
    'time_supersonic_speed',              # percentage equivalent exists
    'time_boost_speed',                   # percentage equivalent exists
    'time_slow_speed',                    # percentage equivalent exists
    'time_ground',                        # percentage equivalent exists
    'time_low_air',                       # percentage equivalent exists
    'time_high_air',                      # percentage equivalent exists
    'time_powerslide',                    # -> count_powerslide and avg_powerslide_duration
    'percent_boost_speed',                # -> percent_slow_speed and percent_supersonic_speed
    'percent_ground',                     # -> percent_low_air and percent_high_air,
    'time_defensive_third',               # percentage equivalent exists
    'time_neutral_third',                 # percentage equivalent exists
    'time_offensive_third',               # percentage equivalent exists
    'time_defensive_half',                # percentage equivalent exists
    'time_offensive_half',                # percentage equivalent exists
    'time_behind_ball',                   # percentage equivalent exists
    'time_infront_ball',                  # percentage equivalent exists
    'time_most_back',                     # percentage equivalent exists
    'time_most_forward',                  # percentage equivalent exists
    'goals_against_while_last_defender',  # always last defender in 1v1
    'time_closest_to_ball',               # percentage equivalent exists
    'time_farthest_from_ball',            # percentage equivalent exists
    'percent_defensive_half',             # -> thirds
    'percent_offensive_half',             # -> thirds
    'percent_neutral_third',              # -> percent_offensive_third and percent_defensive_third
    'percent_infront_ball',               # -> percent_behind_ball
    'percent_most_back',                  # always most back in 1v1
    'percent_most_forward',               # always most forward in 1v1
    'percent_closest_to_ball',            # always closest to ball in 1v1
    'percent_farthest_from_ball',         # always farthest from ball in 1v1
    'avg_distance_to_ball_possession',    # -> avg_distance_to_ball
    'avg_distance_to_ball_no_possession', # -> avg_distance_to_ball
    'taken',                              # reciprocal of inflicted
  ], inplace=True)
```

When we are performing analysis of any data, it doesn't make much sense to have multiple variables
that represent the same thing. When looking for linear relationships between variables, covariance
caused by the fact that some data is represented in multiple places can obscure trends and make
analysis more difficult. This function is designed to remove all of the metrics which are
redundantly representing data or not useful in the context of this analysis. For example:
- `time_zero_boost` is not needed since it is represented by `percent_zero_boost` and `duration`.
- `time_infront_ball` is not needed since it is just `duration` - `time_behind_ball`.
- `shots_against` is not needed since it will be the same as `shots` for the opponent.
- `percent_closest_to_ball` is a team statistic. On a 1-player team, you are always the one closest
to the ball.

### Normalization

```python
# file: "pre_processing.py"
def normalize(data):
  # identify count-like metrics
  clm = [
    # core
    'shots', 'goals', 'saves', 'score',
    # boost
    'amount_collected', 'amount_stolen', 'amount_used_while_supersonic',
    # movement
    'count_powerslide',
    # demo
    'inflicted',
  ]
  # normalize by 5-minute duration
  data[clm] = data[clm].multiply(300 / data['duration'], axis=0)
```

Many of the metrics ballchasing supplies are also given as percentages of the match length (like
`time_zero_boost` and `percent_zero_boost`). However, some metrics (like `goals`) are just a single
number, since they cannot be represented as a percentage. A standard Rocket League match is 5
minutes, but it can be cut short if one team forfeits, and it can go long if it goes into overtime
(matches cannot end in a tie). All of our metrics need to be normalized in order to most
effectively find trends and relationships, so for metrics without percentage equivalents, the data
is scaled such that the duration of the match becomes 300 seconds, or 5 minutes.

### Putting it All Together

```python
# file: "pre_processing.py"
def aggregate():
  # where to store the pickled data
  pickle_path = 'summaries\\data_frame'

  if os.path.exists(pickle_path):
    # if the data has already been aggregated, just read it from the file
    data = pd.read_pickle(pickle_path)
  else:
    # extract all the desired data into one table
    data = pd.concat(dict(iter_stats()), names=('rank', 'guid', 'team'), axis=1)
    # remove the undesired metrics
    prepare_metrics(data)
    # order by rank
    data = data.T.reindex(ranks, level='rank')
    # convert applicable columns to dtype float
    for col in data.columns:
      if col != 'title':
        data[col] = data[col].astype(float)
    # normalize the count-like metrics
    normalize(data)
    data.to_pickle(pickle_path)
  
  return data
```

On my computer, aggregating the data from all of the matches takes about 60 seconds, so instead of
waiting that long every time I want to load the data in a new program, I store it in a file which
can be loaded 100x faster. So, if this is not the first time this method has been called, the `if`
statement will find the file and just read from it. Otherwise, the functions described above are
used to aggregate all of the data. In order to aviod memory fragmentaion in the resulting
`pandas.DataFrame`, I put all of the individual `Series`' into a `dict` first, which has a few
benefits:
- Since the keys of the `dict` are 3-tuples containing metadata about the observation, they will
be used by `pandas.MultiIndex` to allow for [hierarchical indexing][02] of the data table.
- Since it is a `dict`, observations with duplicate keys will be ignored. This is also why I used
the GUID from each replay in the key insead of the ID. The ID of a replay is generated client-side,
and will only be unique to the client. The GUID is generated server-side, and will be *Globally*
unique (hence the *G* in *GUID*). That way, if both players in the match upload the replay to
ballchasing.com and both end up being downloaded, data from only one of the files will be included
in my dataset.

`pandas.concat` takes this dictionary of `Series` objects and puts them all into one table.
However, they are concatenated as columns, so after removing the undesired metrics I take the
transpose. I also reorder them with `DataFrame.reindex` so that they appear (and are iterated
through) in order of increasing rank. Then, I cast each metric (excluding the string `title`) from
`object`s to`float`s, which will mae later calculations simpler and faster. Then I write it to a
file using `pickle` so I won't need to wait next time.

Now it's time to move on to the next step, which is dealing with bias and missing data.

Continue with [Bias & Missing Data](bias_and_missing.md){:.heading.flip-title}
{:.read-more}

[00]: https://vita.had.co.nz/papers/tidy-data.pdf

[01]: /320_FP/

[02]: https://pandas.pydata.org/docs/user_guide/advanced.html